{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646acef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    - NLTK -- for tokenezing russian texts of election programs\n",
    "    \n",
    "    - (LINK!!) -- for creating matrix of program_x_term\n",
    "    - sklearn -- for clustering...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a02442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555a62e3",
   "metadata": {},
   "source": [
    "# 1. Loading data -- table of election programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10feb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19c3463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_x_name</th>\n",
       "      <th>program_txt</th>\n",
       "      <th>convocation_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_Левин_Андрей</td>\n",
       "      <td>Активный участник и организатор различных меро...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_Кириллова_Полина</td>\n",
       "      <td>Мне нравится заниматься общественной деятельно...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_Семенов_Павел</td>\n",
       "      <td>\"Здравствуйте. Я так понимаю, что я уже поздно...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_Кузнецова_Алёна</td>\n",
       "      <td>\"Меня очень интересует жизнь Университета. я б...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_Кунденко_Вадим</td>\n",
       "      <td>\"   Добрый день! Пару слов о себе. Путь к мехм...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          conv_x_name                                        program_txt  \\\n",
       "0      1_Левин_Андрей  Активный участник и организатор различных меро...   \n",
       "1  1_Кириллова_Полина  Мне нравится заниматься общественной деятельно...   \n",
       "2     1_Семенов_Павел  \"Здравствуйте. Я так понимаю, что я уже поздно...   \n",
       "3   1_Кузнецова_Алёна  \"Меня очень интересует жизнь Университета. я б...   \n",
       "4    1_Кунденко_Вадим  \"   Добрый день! Пару слов о себе. Путь к мехм...   \n",
       "\n",
       "   convocation_no  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs_df = pd.read_csv('../library: main/ssmm_elections_candidates_1_11.csv', sep='\\t', encoding='utf-8')\n",
    "programs_df = programs_df[['conv_x_name','program_txt','convocation_no']][programs_df['program_txt'].notna()].reset_index().drop(['index'],axis=1)\n",
    "programs_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e71ae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(programs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebc4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c57ca5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_Левин_Андрей',\n",
       " '1_Кириллова_Полина',\n",
       " '1_Семенов_Павел',\n",
       " '1_Кузнецова_Алёна',\n",
       " '1_Кунденко_Вадим',\n",
       " '1_Сафина_Диана_Ураловна',\n",
       " '1_Глухова_Алёна_Олеговна',\n",
       " '1_Владыкина_Вероника_Евгеньевна',\n",
       " '1_Попов_Леонид_Андреевич',\n",
       " '1_Тропин_Александр_Михайлович',\n",
       " '1_Сапунов_Кирилл_Вячеславович',\n",
       " '1_Артамонова_Мария_Владимировна',\n",
       " '1_Удимов_Даниил_Алексеевич',\n",
       " '1_Котляров_Никита_Владимирович',\n",
       " '1_Чернодед_Антон_Игоревич',\n",
       " '1_Штейников_Юрий_Николаевич',\n",
       " '1_Савушкин_Никита_Максимович',\n",
       " '1_Миценко_Вадим_Валериевич',\n",
       " '2_Остроухова_Наталья_Владимировна',\n",
       " '2_Габидов_Александр_Сергеевич',\n",
       " '2_Дяченко_Мария_Игоревна',\n",
       " '2_Ванунц_Арсений_Ашотович',\n",
       " '2_Ватутин_Кирилл_Александрович',\n",
       " '2_Никифоров_Сергей_Игоревич',\n",
       " '2_Власов_Артем_Андреевич',\n",
       " '2_Медведева_Яна_Григорьевна',\n",
       " '2_Тишин_Павел_Владимирович',\n",
       " '2_Багров_Константин_Владимирович',\n",
       " '2_Быстрицкая_Василина_Васильевна',\n",
       " '2_Сафина_Диана_Ураловна',\n",
       " '2_Лавров_Василий_Алексеевич',\n",
       " '2_Осин_Руслан_Владимирович',\n",
       " '2_Антонов_Сергей_Валентинович',\n",
       " '2_Смелов_Владимир_Павлович',\n",
       " '2_Оджаев_Рахмет_Курбандурдыевич',\n",
       " '2_Владыкина_Вероника_Евгеньевна',\n",
       " '2_Попов_Леонид_Андреевич',\n",
       " '2_Тропин_Александр_Михайлович',\n",
       " '2_Сергеева_Элина_Александровна',\n",
       " '2_Тен_Анна_Бенхиевна',\n",
       " '2_Акушевич_Андрей_Дмитриевич',\n",
       " '2_Дьяков_Павел_Александрович',\n",
       " '2_Сапунов_Кирилл_Вячеславович',\n",
       " '2_Абдуллаева_Эмилия_Табриз_кызы',\n",
       " '2_Котляров_Никита_Владимирович',\n",
       " '2_Морозов_Сергей_Сергеевич']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list = list(programs_df['conv_x_name'])[:46]\n",
    "key_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "072ecf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_list = list(programs_df['program_txt'])[:46]\n",
    "\n",
    "# program_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43435f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c0a2a6",
   "metadata": {},
   "source": [
    "# 2. Tokenizing of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb22fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: importlib-metadata in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from click->nltk) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.4.0)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f1b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5866b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# for russian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5871bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_noise(text, stop_words = []):\n",
    "    tokens = word_tokenize(text, language=\"russian\")\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub('[^А-Яа-я0-9]+', '', token)\n",
    "        if len(token) > 1 and token.lower() not in stop_words:\n",
    "            # Get lowercase\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "# [^A-Za-z0-9]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f83312b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['мне', 'нравится', 'заниматься', 'общественной', 'деятельностью']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove_noise(\"Мне нравится заниматься общественной деятельностью.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524c841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://brandonrose.org/clustering#Stopwords,-stemming,-and-tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e2579bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec43516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add stopwords\n",
    "\n",
    "stopwords += ['это', 'этим', 'свой']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f7097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is just the process of breaking a word down into its root.\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184e386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9075fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents = nltk.sent_tokenize('Мне нравится заниматься общественной деятельностью, всячески пытаюсь расширять свой кругозор. Еще в школе организовывала различные мероприятия и была сильно этим заинтересована', language=\"russian\")\n",
    "# print(sents)\n",
    "\n",
    "# for sent in sents:\n",
    "#     for word in nltk.word_tokenize(sent, language=\"russian\"):\n",
    "#         print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5489d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def tokenize_and_stem(text, stopwords, lang):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text, language=lang) for word in nltk.word_tokenize(sent, language=lang) if word.lower() not in stopwords]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text, stopwords, lang):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text, language=lang) for word in nltk.word_tokenize(sent, language=lang) if word.lower() not in stopwords]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[а-яА-Я]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "236185b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for program in program_list:\n",
    "#     print(tokenize_and_stem(program, stopwords, \"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dfc117a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "for i in program_list:\n",
    "    allwords_stemmed = tokenize_and_stem(i, stopwords, \"russian\") #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i, stopwords, \"russian\")\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99d8f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3107 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289087aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ee16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee86f96",
   "metadata": {},
   "source": [
    "# Tf-idf and program similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_list, program_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8f55960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31ddcf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define vectorizer parameters\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "#                                  min_df=0.2, stop_words=stopwords,\n",
    "#                                  use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "# %time tfidf_matrix = tfidf_vectorizer.fit_transform(program_list) \n",
    "\n",
    "# print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "max_df - float or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher \n",
    "than the given threshold (corpus-specific stop words). \n",
    "If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. \n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df - float or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower \n",
    "than the given threshold. This value is also called cut-off in the literature. \n",
    "If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. \n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features - int, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f6b5069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 1774)\n"
     ]
    }
   ],
   "source": [
    "# alternative way, with won list of stop words\n",
    "# https://stackoverflow.com/questions/26826002/adding-words-to-stop-words-list-in-tfidfvectorizer-in-sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "#     max_df=0.8, max_features=200000, min_df=0.2, \n",
    "    ngram_range=(1,1), stop_words=stopwords\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(program_list)\n",
    "\n",
    "# idf_values = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# printing the tfidf vectors\n",
    "# print(tfidf_matrix)\n",
    "\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf7c1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "# terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751d807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e887d17b",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c2138a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24b26dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.7 ms, sys: 66.1 ms, total: 147 ms\n",
      "Wall time: 39.9 ms\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b69e484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_clusters = {\n",
    "    'key': key_list, \n",
    "    'program': program_list, \n",
    "    'cluster': clusters\n",
    "}\n",
    "\n",
    "frame = pd.DataFrame(program_clusters, index = [clusters] , columns = ['key', 'program', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb48ae22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    16\n",
       "4     9\n",
       "3     8\n",
       "1     7\n",
       "0     6\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeac272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = frame['rank'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "\n",
    "# grouped.mean() #average rank (1 to 100) per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6100d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words:\n",
      "Cluster 0 keys: 1_Владыкина_Вероника_Евгеньевна, 1_Попов_Леонид_Андреевич, 1_Удимов_Даниил_Алексеевич, 2_Ватутин_Кирилл_Александрович, 2_Осин_Руслан_Владимирович, 2_Абдуллаева_Эмилия_Табриз_кызы,\n",
      "Cluster 1 words:\n",
      "Cluster 1 keys: 1_Кириллова_Полина, 1_Чернодед_Антон_Игоревич, 2_Никифоров_Сергей_Игоревич, 2_Медведева_Яна_Григорьевна, 2_Быстрицкая_Василина_Васильевна, 2_Сафина_Диана_Ураловна, 2_Тен_Анна_Бенхиевна,\n",
      "Cluster 2 words:\n",
      "Cluster 2 keys: 1_Кунденко_Вадим, 1_Тропин_Александр_Михайлович, 1_Сапунов_Кирилл_Вячеславович, 1_Артамонова_Мария_Владимировна, 1_Котляров_Никита_Владимирович, 1_Савушкин_Никита_Максимович, 2_Ванунц_Арсений_Ашотович, 2_Власов_Артем_Андреевич, 2_Тишин_Павел_Владимирович, 2_Багров_Константин_Владимирович, 2_Смелов_Владимир_Павлович, 2_Оджаев_Рахмет_Курбандурдыевич, 2_Владыкина_Вероника_Евгеньевна, 2_Тропин_Александр_Михайлович, 2_Сергеева_Элина_Александровна, 2_Котляров_Никита_Владимирович,\n",
      "Cluster 3 words:\n",
      "Cluster 3 keys: 1_Левин_Андрей, 1_Семенов_Павел, 1_Кузнецова_Алёна, 1_Сафина_Диана_Ураловна, 1_Глухова_Алёна_Олеговна, 2_Остроухова_Наталья_Владимировна, 2_Габидов_Александр_Сергеевич, 2_Дяченко_Мария_Игоревна,\n",
      "Cluster 4 words:\n",
      "Cluster 4 keys: 1_Штейников_Юрий_Николаевич, 1_Миценко_Вадим_Валериевич, 2_Лавров_Василий_Алексеевич, 2_Антонов_Сергей_Валентинович, 2_Попов_Леонид_Андреевич, 2_Акушевич_Андрей_Дмитриевич, 2_Дьяков_Павел_Александрович, 2_Сапунов_Кирилл_Вячеславович, 2_Морозов_Сергей_Сергеевич,\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "#     for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "#         print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "#     print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d keys:\" % i, end='')\n",
    "    for title in frame.loc[i]['key'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "#     print() #add whitespace\n",
    "    \n",
    "# print()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d4370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
